\section{Our technique}

\subsection{Basic technique}

To illustrate our technique, we first show a very simple version of it
in the form of the following code:

{\small\begin{verbatim}
(defun count-from-end (x list)
  (labels ((aux (x list n)
             (cond ((= n 0) 0)
                   ((= n 1)
                    (if (eq x (car list)) 1 0))
                   (t (let* ((n/2 (ash n -1))
                             (half (nthcdr n/2 list)))
                        (+ (aux x half (- n n/2))
                           (aux x list n/2)))))))
    (aux x list (length list))))))
\end{verbatim}}

This function starts by computing the length of the list and then
calling the auxiliary function with the original arguments and the
length.  The auxiliary function calls \texttt{nthcdr} in order to get
a reference to about half the list it was passed.  Then it makes two
recursive calls, first with the second half of the list and then with
the firs half of the list.  The recursion terminates when the list has
a single element or no elements in it.  When it has no element in it,
clearly the count is $0$.  When it has a single element in it, the
element is compared to the argument \texttt{x} and if they are the
same, the value $1$ is returned.  Otherwise $0$ is returned.

The main feature of our technique is that it trades fewer recursive
calls for multiple traversals of the list.  The maximum number% of
simultaneous active invocations of this simple function is around
$\mathsf{lb}\thinspace n$, where $n$ is the length of the list.  The
maximum value of this number is quite modest.  On a 64-bit processor,
it can never exceed $60$ and it is significantly smaller in practice
of course.  The number of times this function computes the
\texttt{cdr} of a list depends on where in the list the item to be
found is located.  If it is the \emph{last} element of the list (best
case), each \texttt{cons} cell is processed twice; once to compute the
length of the list, and once again as part of the recursive traversal.
When the item to be found is the \emph{first} element of the list
(worst case), the number of \texttt{cdr} operations can be
approximately expressed as $n\thinspace (1 +
\frac{1}{2}\mathsf{lb}\thinspace n)$.

The best case for this function is very efficient indeed.%
\FIXME{Add a table comparing execution times}
The worst case is unacceptably slow.  Even for a list of some
reasonable length such as a million elements, the execution time is a
factor $20$ slower than for the best case.

The remainder of this section is dedicated to ways of improving on the
worst-case performance of the basic technique.

\subsection{Using more stack space}

By far the most important improvement to the basic technique is to
take advantage of the available stack space to decrease the number of
multiple list traversals required by the basic technique.

\FIXME{Pourquoi pas un seul "labels"?  Because \texttt{recursive} does
  not need to call \texttt{aux}}

{\small\begin{verbatim}
(defun find-2 (x list)
  (labels ((recursive (x list n)
             (if (zerop n)
                 nil
                 (progn (recursive x (cdr list) (1- n))
                        (when (eq x (car list))
                          (return-from find-2 x))))))
    (labels ((aux (x list n)
               (if (< n 10000)
                   (recursive x list n)
                   (let* ((n/2 (ash n -1))
                          (half (nthcdr n/2 list)))
                     (aux x half (- n n/2))
                     (aux x list n/2)))))
      (aux x list (length list)))))
\end{verbatim}}

\subsection{Other improvements}

Excluding the initial traversal of the list in order to compute the
length, the number of \texttt{cdr} operations of the basic technique
can be expressed with this recursive equation:%
\FIXME{Add the base case.}

$$f(n) = \left\lfloor\frac{n}{2}\right\rfloor
+ f(\left\lfloor\frac{n}{2}\right\rfloor)4.
+ f(\left\lceil\frac{n}{2}\right\rceil)$$

\subsection{Implementation-specific solutions}

So far, we have explored techniques that can mostly be implemented in
portable \commonlisp{}.  In this section, we explore a variation on
our technique that requires access to the control stack of the
implementation.

Recall that at the lowest level of our technique, there is a recursive
function that is used for traversing the list when the number of
elements is small compared to the stack size.  At each invocation,
this function does very little work.

With direct access to the control stack, we can convert the recursive
function to an iterative function that pushes the elements of the list
on the control stack, and then processes them in reverse order.  This
technique has several advantages:

\begin{itemize}
\item A single word is used for each element, whereas the recursive
  function requires space for a return address, a frame pointer,
  saved registers, etc.  As a result, this technique can be used for
  lists with more elements than would be possible with the recursive
  technique, thereby further decreasing the number of times a list is
  traversed.
\item There is no function-call overhead involved.  The only
  processing that is needed for an element is to store it on the stack
  and then comparing it to the item.
\end{itemize}

We illustrate this technique in a notation similar to \commonlisp{}:

\begin{verbatim}
(defun low-level-reverse-count (item list length)
  (loop for rest = list then (cdr rest)
        repeat length
        do (push-on-stack (car rest)))
  (loop repeat length
        count (eq item (pop-from-stack))))
\end{verbatim}

We implemented this technique in SBCL.  In order not to have to
recompile SBCL with our additional function, we used the
implementation-specific foreign-function interface and wrote the
function using the language C.  Rather than pushing and popping the
control stack, we used the built-in C function \texttt{alloca} to
allocate a one-dimensional C array on the control stack to hold the
list elements.

In SBCL, the default stack size is $2$MBytes, or around $250$k words
on a 64-bit processor.  We tested our technique using $100000$ words
on the stack.  The result is that for a list with $10$ million
elements, our technique processes the list in reverse order as fast as
an ordinary loop from the beginning of the list.

This surprising result can be explained by a few factors:

\begin{itemize}
\item Presumably in order to speed up the functions \texttt{car} and
  \texttt{cdr}, SBCL uses the same tag for \texttt{cons} cells and for
  the symbol \texttt{nil}.  As a result, in order to traverse a list,
  SBCL must make \emph{two} tests for each element, namely one to
  check whether the putative list is something other than a list
  altogether, and another to check whether it is a \texttt{cons}
  cell.
\item The SBCL compiler can not determine that the return value of
  \texttt{count} must always be a \texttt{fixnum}.%
  \footnote{In implementation with more tag bits, the result of
    \texttt{count} might be a \texttt{bignum}, but in SBCL, a single
    bit is used for a \texttt{fixnum} tag, and there can not be as
    many \texttt{cons} cells in the system as the largest possible
    \texttt{fixnum}.}
  When the function is implemented in C, this problem disappears.
\end{itemize}
